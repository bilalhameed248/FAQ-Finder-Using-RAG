{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671d1ab4-5547-4fc5-913c-9eb596fcc7d8",
   "metadata": {},
   "source": [
    "## What is RAG (Retrieval-Augmented Generation)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5817a279-76ff-4217-bf01-68879402750c",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is a method that combines the strengths of retrieval-based and generative models to improve the performance of natural language understanding and generation tasks. RAG models use a retriever to fetch relevant documents from a large corpus and then use a generative model to produce answers or generate text based on the retrieved documents.\n",
    "<br><br>\n",
    "The typical workflow for RAG involves the following steps: <br>\n",
    "<ul>\n",
    "<li>Query Processing: A query is given to the system.</li>\n",
    "<li>Retrieval: The retriever searches a large corpus to find the most relevant documents related to the query.</li>\n",
    "<li>Generation: The generator uses the retrieved documents to generate a coherent and relevant response.</li>\n",
    "<ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c069b-d93d-41c3-9a14-b6c71edf9d85",
   "metadata": {},
   "source": [
    "## Implementing RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf731858-09b2-4718-9b64-72765f3eb87d",
   "metadata": {},
   "source": [
    "Implementing a RAG model involves two main components:\n",
    "\n",
    "<ul>\n",
    "<li>Retriever: This component fetches relevant documents from a large dataset. Common retrieval models include BM25, TF-IDF, and dense vector retrievers like those based on BERT embeddings.</li>\n",
    "<li>Generator: This component generates the final output using the retrieved documents. Generative models like GPT-3, T5, or BART can be used.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "92592d8a-d614-4746-a839-8ee69875550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938c421-220a-4331-a61a-0aad9af2ceed",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "40879c56-79cd-4b6c-a663-782bb2d9dfb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Questions</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is RAG?</td>\n",
       "      <td>RAG stands for Retrieval-Augmented Generation.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does RAG work?</td>\n",
       "      <td>RAG works by combining retrieval-based and gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the components of RAG?</td>\n",
       "      <td>RAG has two main components: a retriever and a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Questions  \\\n",
       "0                     What is RAG?   \n",
       "1               How does RAG work?   \n",
       "2  What are the components of RAG?   \n",
       "\n",
       "                                             Answers  \n",
       "0     RAG stands for Retrieval-Augmented Generation.  \n",
       "1  RAG works by combining retrieval-based and gen...  \n",
       "2  RAG has two main components: a retriever and a...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your DataFrame\n",
    "data = {'Questions': [\"What is RAG?\", \"How does RAG work?\", \"What are the components of RAG?\"],\n",
    "        'Answers': [\"RAG stands for Retrieval-Augmented Generation.\",\n",
    "                    \"RAG works by combining retrieval-based and generative models.\",\n",
    "                    \"RAG has two main components: a retriever and a generator.\"]}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec459246-ad10-4b89-b047-9ea133518769",
   "metadata": {},
   "source": [
    "## Loading FAQ-Chat-Bot Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512d316a-e161-4739-a8e8-faf687de6e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Question_Variations_Embeddings_Updated.csv')\n",
    "df = df[['Question', 'Answer']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4624d-a866-4aee-a358-bf32134850b4",
   "metadata": {},
   "source": [
    "## Encode Questions for Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ef4cab-b700-4807-a28b-fa6c6031f492",
   "metadata": {},
   "source": [
    "Use a model like SentenceTransformer to encode the questions into dense vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e303bcdd-80b8-4a98-a8f6-dd4c76cfd138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sharjeelahmed/datascience/gpu_env/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained SentenceTransformer model\n",
    "retriever = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# # Encode the questions from the DataFrame\n",
    "# question_embeddings = retriever.encode(df['Question'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# # Save corpus embeddings for future use\n",
    "# torch.save(question_embeddings, 'question_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238e01c-b360-423b-89c4-cf4b156aa826",
   "metadata": {},
   "source": [
    "## Load the saved corpus embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ad18fbcb-3f50-42a8-a6c0-0f16f95594ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings = torch.load('question_embeddings.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81524bc4-f164-404b-8922-5a7894be4c00",
   "metadata": {},
   "source": [
    "## Set Up the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d3065-4f86-41d2-93c5-880645c62223",
   "metadata": {},
   "source": [
    "For generating answers, you can use a model like T5 or GPT-3. Here we will use T5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "61dd4c22-b752-4d42-875e-a79544dbfb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load T5 model and tokenizer\n",
    "model_name = 't5-base'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "generator = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b8c36-a41e-4803-8705-b076c3dd7691",
   "metadata": {},
   "source": [
    "## Define the Retrieval and Generation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1c462a-eff3-4c67-9d26-3bbb680ccea8",
   "metadata": {},
   "source": [
    "Combine the retriever and generator into a pipeline to handle user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "43e7f28b-ce10-4988-baa6-f409cb343f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "def retrieve_answer(query, retriever, question_embeddings, df, top_k=1):\n",
    "    # Encode the query\n",
    "    query_embedding = retriever.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # Retrieve top_k most similar questions\n",
    "    hits = util.semantic_search(query_embedding, question_embeddings, top_k=top_k)\n",
    "    print(\"hits\",hits)\n",
    "    hits = hits[0]  # Get the list of hits for the first query\n",
    "\n",
    "    # Get the corresponding answer(s) from the DataFrame\n",
    "    retrieved_answers = [df.iloc[hit['corpus_id']]['Answer'] for hit in hits]\n",
    "    retrieved_questions = [df.iloc[hit['corpus_id']]['Question'] for hit in hits]\n",
    "\n",
    "    return retrieved_questions, retrieved_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e8693-5f94-497f-94e9-d58ad947a757",
   "metadata": {},
   "source": [
    "## Use t5 to Generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "6cf59e46-a377-45cc-b3a9-b5181dd96c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_using_t5(query, retrieved_answers, tokenizer, generator):\n",
    "    # If generative step is needed, combine the retrieved answers\n",
    "    input_text = query + \" \" + \" \".join(retrieved_answers)\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', max_length=512, truncation=True)\n",
    "    # Generate the answer\n",
    "    output_sequences = generator.generate(\n",
    "        input_ids=inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=150,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    # Decode the generated text\n",
    "    generated_answer = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    return generated_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929d2b36-e8de-4a2c-af51-f25d541e049a",
   "metadata": {},
   "source": [
    "## Perplexity.ai To generate Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5b1da9a8-df04-4e60-8598-12fc3cc70b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "def generate_answer_using_prplx(retrieved_answers):\n",
    "    YOUR_API_KEY = \"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an artificial intelligence assistant and you need to \"\n",
    "                \"engage in a helpful, detailed, polite conversation with a user.\"\n",
    "            ),\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                f\"\"\"{\"Repherase the following text: \"+ \" \".join(retrieved_answers)}\"\"\"\n",
    "            ),\n",
    "        },\n",
    "    ]\n",
    "    client = OpenAI(api_key=YOUR_API_KEY, base_url=\"https://api.perplexity.ai\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3-sonar-large-32k-online\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201329ef-70e4-46b5-b2f7-b04137b755bb",
   "metadata": {},
   "source": [
    "## LLAMA-3 8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "836859eb-15b6-4a86-8536-626987918030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def llama3(retrieved_answers):\n",
    "    url = \"http://localhost:11434/api/chat\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Rephrase the following text: {' '.join(retrieved_answers)}\"\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"message\", {}).get(\"content\", \"\")\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67641e0-8888-412d-b15e-e09f88b94b7b",
   "metadata": {},
   "source": [
    "## Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0376c537-6d6f-46c5-85fe-c8e576ff07a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hits [[{'corpus_id': 6719, 'score': 0.2754690647125244}]]\n"
     ]
    }
   ],
   "source": [
    "query = \"what is the weather outside?\"\n",
    "# Retrieve answer\n",
    "retrieved_questions, retrieved_answers = retrieve_answer(query, retriever, question_embeddings, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "12db969d-1576-4801-8d9a-79179e76410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: what is the weather outside? \n",
      "\n",
      "Retrieved Questions: ['What is up, bro?'] \n",
      "\n",
      "Retrieved Answers: ['Hi! How can I assist you today?'] \n",
      "\n",
      "********************************************************************************************************************\n",
      "Generated Answer: Here's a rephrased version:\n",
      "\n",
      "\"Welcome! What brings you here today? I'm happy to help with whatever you need.\"\n"
     ]
    }
   ],
   "source": [
    "#Generate Answer\n",
    "# answer = generate_answer_using_t5(query, retrieved_answers, tokenizer, generator)\n",
    "# answer = generate_answer_using_prplx(retrieved_answers)\n",
    "answer = llama3(retrieved_answers)\n",
    "\n",
    "\n",
    "print(f\"Query: {query} \\n\")\n",
    "print(f\"Retrieved Questions: {retrieved_questions} \\n\")\n",
    "print(f\"Retrieved Answers: {retrieved_answers} \\n\")\n",
    "\n",
    "print(\"********************************************************************************************************************\")\n",
    "\n",
    "print(f\"Generated Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0acf9-5538-4057-8c3b-4cefb339a24c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5a44e-c643-43f8-9e4b-bd633c2ce337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3ca91-c51d-4160-bf6b-a12a963249e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b625cf8-b1d3-4438-b303-067294ac4712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501f3b1-f178-42fe-87f7-1097893e0238",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e015c-29ff-4de9-a5ea-770b441d0632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326ee2e-ea73-4814-b834-769fbdb6201f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3605d001-08de-46f2-9544-8ee2752019a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9295ed97-a6eb-4a81-8c68-6bf6f96075db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
